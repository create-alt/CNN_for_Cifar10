{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/create-alt/CNN_for_Cifar10/blob/main/CNN_for_Cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CNNによるCifar-10分類**"
      ],
      "metadata": {
        "id": "Ou2DXcx1Id9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "環境：<br>\n",
        "Google Colaboratory<br>\n",
        "T4 GPU<br>\n",
        "必要ライブラリ等はコード内で記述"
      ],
      "metadata": {
        "id": "EZsmI5kENpKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU確認とGoogleDrive接続"
      ],
      "metadata": {
        "id": "o1sDTBx6w62m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルの学習時にGPUが必要となるので接続を確認しておく。"
      ],
      "metadata": {
        "id": "ulxbT30IBsip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "metadata": {
        "id": "GgbJf2hIei19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習した結果の重みをGoogleDriveに保存するため接続しておく。"
      ],
      "metadata": {
        "id": "S1k0llkhB2ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0QzkisElItVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd {\"/content/drive/MyDrive/重みを格納するフォルダ\"}"
      ],
      "metadata": {
        "id": "Jalg-SXuWY1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ライブラリのimportと定数・関数定義"
      ],
      "metadata": {
        "id": "OBzfBQjgw0u5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8aFTxZIdvSk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import os\n",
        "import cv2\n",
        "import gc\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.image import ImageDataGenerator,array_to_img,img_to_array\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Conv2D, Conv3D, MaxPooling2D, MaxPooling3D, AveragePooling2D, AveragePooling3D\n",
        "\n",
        "label_dict = {0:\"飛行機\", 1:\"自動車\", 2:\"鳥\", 3:\"猫\", 4:\"鹿\", 5:\"犬\", 6:\"カエル\", 7:\"馬\", 8:\"船\", 9:\"トラック\"} #label定義\n",
        "\n",
        "#定数定義（class作成時にまとめられる）\n",
        "NUM_CLASSES = 10\n",
        "SEED=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2YxLRPWdvSs"
      },
      "outputs": [],
      "source": [
        "#データを読み込む\n",
        "def load():\n",
        "  (X_train,y_train),(X_test,y_test) = cifar10.load_data()\n",
        "  print(\"default X_train's shape is\",X_train.shape)\n",
        "  print(\"default y_train's shape is\",y_train.shape,\"\\n\")\n",
        "\n",
        "  # 入力データ[0, 1]の範囲に正規化\n",
        "  X_train = X_train.astype('float32')\n",
        "  X_test = X_test.astype('float32')\n",
        "  # 255で割ったものを新たに変数とする->画素の最大が255なので、255で割ると[0, 1]になる\n",
        "  X_train /= 255\n",
        "  X_test /= 255\n",
        "\n",
        "  #以下データ拡張\n",
        "\n",
        "  #seed値固定\n",
        "  tf.random.set_seed(SEED)\n",
        "\n",
        "  params = {\"zoom_range\" : 0.3,\n",
        "          \"horizontal_flip\" : True,\n",
        "          \"rotation_range\" : 30,\n",
        "          \"height_shift_range\": 0.1,\n",
        "          \"width_shift_range\": 0.1,\n",
        "          \"channel_shift_range\": 0.3}\n",
        "\n",
        "  #データ拡張用オブジェクトの定義\n",
        "  data_generator = keras.preprocessing.image.ImageDataGenerator(**params)\n",
        "\n",
        "  # 同じ画像を複製する\n",
        "  train_aug = X_train.copy()\n",
        "  train_aug = np.concatenate([train_aug,train_aug])\n",
        "\n",
        "  y_aug = np.concatenate([y_train,y_train])\n",
        "\n",
        "  #データ拡張\n",
        "  generator = data_generator.flow(train_aug, y_aug, batch_size=100000)\n",
        "\n",
        "  # 変換後のデータを取得\n",
        "  batch_x = generator.next()\n",
        "\n",
        "  X_train = np.concatenate([X_train,batch_x[0]])\n",
        "  y_train = np.concatenate([y_train,batch_x[1]])\n",
        "\n",
        "  print(X_train.shape)\n",
        "\n",
        "  return (X_train,y_train),(X_test,y_test)\n",
        "\n",
        "#plot用関数(教材から引用)\n",
        "def plot_history_of_model(model):\n",
        "    # 学習をグラフ化（正解率）\n",
        "    print(model.history.history)\n",
        "    acc = model.history.history['accuracy']\n",
        "    val_acc = model.history.history['val_accuracy']\n",
        "\n",
        "    # Accuracy Plot\n",
        "    plt.plot(acc)\n",
        "    plt.plot(val_acc)\n",
        "    plt.ylim(0.0,1.0)\n",
        "    plt.title('Accuracy')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    # Loss Plot\n",
        "    loss = model.history.history['loss']\n",
        "    val_loss = model.history.history['val_loss']\n",
        "    plt.plot(loss ,label = 'training loss')\n",
        "    plt.plot(val_loss, label= 'validation loss')\n",
        "    #plt.ylim(0,3)\n",
        "    plt.title('Training and Validation loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "#checkpointの設定\n",
        "def create_checkpoint(path):\n",
        "  #path(TensorFlow公式サイトより引用 https://www.tensorflow.org/tutorials/keras/save_and_load?hl=ja)\n",
        "\n",
        "  #重みの格納場所(公式サイトでは.ckptを使用していたが安全性の面から.safetensorsを使用)\n",
        "  checkpoint_path = \"Weight_of_model/\" + path + \".safetensors\"\n",
        "  checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "  cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                  save_weights_only=True,\n",
        "                                                  verbose=1)\n",
        "\n",
        "  return checkpoint_path,cp_callback\n",
        "\n",
        "#3次元データ用のニューラルネットワーク\n",
        "def create_3Dmodel():\n",
        "  #seed値固定\n",
        "  tf.random.set_seed(SEED)\n",
        "  # マスクを適応したカラー画像に関するモデル\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv3D(64, kernel_size=(3, 3, 1),\n",
        "                                padding=\"SAME\",\n",
        "                                activation='relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Conv3D(64, (3, 3, 1), padding=\"SAME\",activation='relu'))\n",
        "  model.add(MaxPooling3D(pool_size=(3, 3, 1))) # max pooling layer\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv3D(128, (3, 3, 1), padding=\"SAME\" ,activation='relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Conv3D(128, (3, 3, 1), padding=\"SAME\" ,activation='relu'))\n",
        "  model.add(MaxPooling3D(pool_size=(3, 3, 1))) # max pooling layer\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Flatten())\n",
        "\n",
        "  model.add(Dense(256, activation='relu')) # 全結合層\n",
        "  model.add(Dense(NUM_CLASSES, activation='softmax')) #classごとに確率を出力\n",
        "\n",
        "  # 損失関数,最適化関数,評価指標を指定してモデルをコンパイル->学習できる形にする\n",
        "  model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "          #optimizer=keras.optimizers.Adadelta(),\n",
        "          optimizer=tf.keras.optimizers.Adam(),\n",
        "          metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "#2次元データ用のニューラルネットワーク\n",
        "def create_2Dmodel(input_size):\n",
        "  #seed値固定\n",
        "  tf.random.set_seed(SEED)\n",
        "  # グレースケール化した画像に対するモデル\n",
        "  model = Sequential()\n",
        "  #addで層を追加する,↓だとconv2dを追加\n",
        "  model.add(Conv2D(64, kernel_size=(3, 3),padding=\"SAME\",\n",
        "                        activation='relu',\n",
        "                        input_shape=input_size)) # 3×3のカーネルサイズの2D Convolution layer\n",
        "  model.add(Conv2D(64, (3, 3),padding=\"SAME\", activation='relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3),padding=\"SAME\", activation='relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Conv2D(64, (3, 3),padding=\"SAME\", activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv2D(128, (3,3),padding=\"SAME\",activation='relu'))\n",
        "  model.add(Conv2D(128, (3,3),padding=\"SAME\",activation='relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(256, activation='relu')) # 全結合層\n",
        "  model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "  # モデルの学習\n",
        "  # 損失関数,最適化関数,評価指標を指定してモデルをコンパイル->学習できる形にする\n",
        "  model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                            #optimizer=keras.optimizers.Adadelta(),\n",
        "                            optimizer=tf.keras.optimizers.Adam(),\n",
        "                            metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 分類モデルの構築と学習"
      ],
      "metadata": {
        "id": "NsicgcbY8_Uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train,y_train),(X_test,y_test) = load()\n",
        "#画像サイズ指定\n",
        "img_rows, img_cols = X_train.shape[1], X_train.shape[2]"
      ],
      "metadata": {
        "id": "-EmjbEF215xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次のコードでは特に分類精度の低かった猫データについて追加拡張を行っている。"
      ],
      "metadata": {
        "id": "hJfrnhgbLnIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_cat = X_train[np.where(y_train == 3)[0]]\n",
        "\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "params = {\"zoom_range\" : 0.3,\n",
        "        \"horizontal_flip\" : True,\n",
        "        \"rotation_range\" : 30,\n",
        "        \"height_shift_range\": 0.1,\n",
        "        \"width_shift_range\": 0.1,\n",
        "        \"channel_shift_range\": 0.3}\n",
        "\n",
        "#データ拡張用オブジェクトの定義\n",
        "data_generator = keras.preprocessing.image.ImageDataGenerator(**params)\n",
        "\n",
        "# 同じ画像を複製する\n",
        "train_aug = X_cat.copy()\n",
        "\n",
        "y_aug = y_train[np.where(y_train == 3)[0]]\n",
        "\n",
        "#データ拡張\n",
        "generator = data_generator.flow(train_aug, y_aug, batch_size=15000)\n",
        "\n",
        "# 変換後のデータを取得\n",
        "batch_x = generator.next()\n",
        "\n",
        "X_train = np.concatenate([X_train,batch_x[0]])\n",
        "y_train = np.concatenate([y_train,batch_x[1]])\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "KutGtGjBqYIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e46J9rCdvSr"
      },
      "outputs": [],
      "source": [
        "n_split=3 #いくつに分割するか（何度学習するか）今回は1度のみ学習します\n",
        "kf = StratifiedKFold(n_split,shuffle=True,random_state=0)\n",
        "split_index=[[] for i in range(n_split)] #分割後のindexを格納するためのlist\n",
        "\n",
        "#train_indexは学習用データのindex,valid_indexはtest用データのindex(X,y共用)\n",
        "for fold,(train_index, valid_index) in enumerate(kf.split(X_train,y_train)):\n",
        "    split_index[fold] = [train_index,valid_index]\n",
        "\n",
        "print(split_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMFIaUXc9acQ"
      },
      "outputs": [],
      "source": [
        "#データをkerasが読み取れる形式に変換する(one-hotベクトル)\n",
        "y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
        "print(\"y_train's shape after change to one-hot vec is\",y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgw4kKURdvSu"
      },
      "outputs": [],
      "source": [
        "X_train = np.expand_dims(X_train, axis=-1)  # 最後の次元にチャンネル数を追加\n",
        "X_test = np.expand_dims(X_test, axis=-1)  # 最後の次元にチャンネル数を追加"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path, cp_callback = create_checkpoint(\"重みを保存するファイルのパスをここに記載\")"
      ],
      "metadata": {
        "id": "M5Exf-I2soHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA1wkflYdvSv"
      },
      "outputs": [],
      "source": [
        "NUMBER_OF_EPOCH = 32\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "model_color = create_3Dmodel()\n",
        "\n",
        "# モデルの学習\n",
        "model_color.fit(X_train[split_index[0][0]], y_train[split_index[0][0]],\n",
        "                batch_size=BATCH_SIZE,\n",
        "                epochs=NUMBER_OF_EPOCH,\n",
        "                verbose=1,\n",
        "                validation_data=(X_train[split_index[0][1]], y_train[split_index[0][1]]),\n",
        "                callbacks=[cp_callback])\n",
        "\n",
        "plot_history_of_model(model_color)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#保存した重みの読み込み\n",
        "model_color = create_3Dmodel()\n",
        "\n",
        "model_color.load_weights(checkpoint_path)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nSdO9DdDtL-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7UfQgAjdvSw"
      },
      "outputs": [],
      "source": [
        "pred_color = model_color.predict(X_test)\n",
        "count = 0\n",
        "for i in range(len(X_test)):\n",
        "    if np.argmax(pred_color[i]) == np.argmax(y_test[i]):\n",
        "        count +=1\n",
        "\n",
        "print(\"accuracy :\",count/len(X_test))\n",
        "\n",
        "#accuracy : 0.8055"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "negative_list = np.zeros(10)\n",
        "pred_list = np.zeros(10)\n",
        "\n",
        "true = np.zeros(10)\n",
        "pred = np.zeros(10)\n",
        "\n",
        "my_matrix = np.zeros((10,10))\n",
        "\n",
        "for i in range(len(pred_color)):\n",
        "    if np.argmax(pred_color[i]) == np.argmax(y_test[i]):\n",
        "        count +=1\n",
        "    else:\n",
        "      negative_list[np.argmax(y_test[i])] += 1\n",
        "      pred_list[np.argmax(pred_color[i])] += 1\n",
        "\n",
        "    true[np.argmax(y_test[i])] += 1\n",
        "    pred[np.argmax(pred_color[i])] += 1\n",
        "\n",
        "    my_matrix[np.argmax(y_test[i])][np.argmax(pred_color[i])] += 1\n",
        "\n",
        "print(\"accuracy :\",count/len(pred_color))\n",
        "print(label_dict)\n",
        "print(negative_list)\n",
        "print(pred_list,\"\\n\")\n",
        "\n",
        "print(true)\n",
        "print(pred,\"\\n\")\n",
        "\n",
        "print(my_matrix)"
      ],
      "metadata": {
        "id": "5-zDRgdr4WvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train,X_test\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "Dv6BxkDRwh6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 元データにマスク処理を施したデータに対する分類モデル"
      ],
      "metadata": {
        "id": "dC0izHOlwq70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train,y_train),(X_test,y_test) = load()"
      ],
      "metadata": {
        "id": "oKxO2UFOvY38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_cat = X_train[np.where(y_train == 3)[0]]\n",
        "\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "params = {\"zoom_range\" : 0.3,\n",
        "        \"horizontal_flip\" : True,\n",
        "        \"rotation_range\" : 30,\n",
        "        \"height_shift_range\": 0.1,\n",
        "        \"width_shift_range\": 0.1,\n",
        "        \"channel_shift_range\": 0.3}\n",
        "\n",
        "#データ拡張用オブジェクトの定義\n",
        "data_generator = keras.preprocessing.image.ImageDataGenerator(**params)\n",
        "\n",
        "# 同じ画像を複製する\n",
        "train_aug = X_cat.copy()\n",
        "\n",
        "y_aug = y_train[np.where(y_train == 3)[0]]\n",
        "\n",
        "#データ拡張\n",
        "generator = data_generator.flow(train_aug, y_aug, batch_size=15000)\n",
        "\n",
        "# 変換後のデータを取得\n",
        "batch_x = generator.next()\n",
        "\n",
        "X_train = np.concatenate([X_train,batch_x[0]])\n",
        "y_train = np.concatenate([y_train,batch_x[1]])\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "pwRWhnE8JvZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次のコードでは画像へマスク処理を施している。"
      ],
      "metadata": {
        "id": "2muNN4XqMDj_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl3NjafbdvSo"
      },
      "outputs": [],
      "source": [
        "row, col = np.ogrid[:img_rows, :img_cols]\n",
        "\n",
        "cnt_row, cnt_col = img_rows / 2, img_cols / 2 # Center of the disk\n",
        "outer_disk_mask = ((row - cnt_row)**2 + (col - cnt_col)**2 > (img_cols / 2)**2)\n",
        "\n",
        "print(outer_disk_mask.shape)\n",
        "\n",
        "X_train_masked = X_train.copy()\n",
        "X_test_masked = X_test.copy()\n",
        "for i in range(X_train.shape[0]):\n",
        "    X_train_masked[i][outer_disk_mask] = 0\n",
        "for i in range(X_test.shape[0]):\n",
        "    X_test_masked[i][outer_disk_mask] = 0\n",
        "\n",
        "#メモリ開放\n",
        "del X_train,row,col,cnt_row,cnt_col\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0FHEBp5dvSs"
      },
      "outputs": [],
      "source": [
        "#データをkerasが読み取れる形式に変換する(one-hotベクトル)\n",
        "y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
        "print(\"y_train's shape after change to one-hot vec is\",y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9N10NEOdvSu"
      },
      "outputs": [],
      "source": [
        "X_train_masked = np.expand_dims(X_train_masked, axis=-1)  # 最後の次元にチャンネルを追加\n",
        "X_test_masked = np.expand_dims(X_test, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checkpointの設定\n",
        "checkpoint_path,cp_callback = create_checkpoint(\"重みを保存するファイルのパスをここに記載\")"
      ],
      "metadata": {
        "id": "rJSUD4x5vLSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL5Y4VKcdvSv"
      },
      "outputs": [],
      "source": [
        "#学習時には実行する\n",
        "NUMBER_OF_EPOCH = 32\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "model_color_masked = create_3Dmodel()\n",
        "\n",
        "# モデルの学習\n",
        "model_color_masked.fit(X_train_masked[split_index[0][0]], y_train[split_index[0][0]],\n",
        "                      batch_size=BATCH_SIZE,\n",
        "                      epochs=NUMBER_OF_EPOCH,\n",
        "                      verbose=1,\n",
        "                      validation_data=(X_train_masked[split_index[0][1]], y_train[split_index[0][1]]),\n",
        "                      callbacks=[cp_callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history_of_model(model_color_masked)"
      ],
      "metadata": {
        "id": "8Cu7rL-W3duZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#保存した重みを読み込み、精度を出力\n",
        "model_color_masked = create_3Dmodel()\n",
        "\n",
        "model_color_masked.load_weights(checkpoint_path)\n",
        "\n",
        "loss, acc = model_color_masked.evaluate(X_test_masked, y_test, verbose=2)\n",
        "print(\"Untrained model, accuracy: {:5.2f}%\".format(100 * acc))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "W1PyWuuPqVjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train_masked\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "uQ0Cv1RjvDKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_color_masked = model_color_masked.predict(X_test_masked)\n",
        "\n",
        "count = 0\n",
        "for i in range(len(X_test_masked)):\n",
        "    if np.argmax(pred_color_masked[i]) == np.argmax(y_test[i]):\n",
        "        count +=1\n",
        "\n",
        "print(\"accuracy :\",count/len(X_test_masked))"
      ],
      "metadata": {
        "id": "t-E_g225rD_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_test_masked\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "l5jBONoe0_aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# グレースケール化した画像にマスクを適応させた画像についての分類モデル"
      ],
      "metadata": {
        "id": "E9753zVX5Q_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train,y_train),(X_test,y_test) = load()"
      ],
      "metadata": {
        "id": "-FPRs6M905Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_cat = X_train[np.where(y_train == 3)[0]]\n",
        "\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "params = {\"zoom_range\" : 0.3,\n",
        "        \"horizontal_flip\" : True,\n",
        "        \"rotation_range\" : 30,\n",
        "        \"height_shift_range\": 0.1,\n",
        "        \"width_shift_range\": 0.1,\n",
        "        \"channel_shift_range\": 0.3}\n",
        "\n",
        "#データ拡張用オブジェクトの定義\n",
        "data_generator = keras.preprocessing.image.ImageDataGenerator(**params)\n",
        "\n",
        "# 同じ画像を複製する\n",
        "train_aug = X_cat.copy()\n",
        "\n",
        "y_aug = y_train[np.where(y_train == 3)[0]]\n",
        "\n",
        "#データ拡張\n",
        "generator = data_generator.flow(train_aug, y_aug, batch_size=15000)\n",
        "\n",
        "# 変換後のデータを取得\n",
        "batch_x = generator.next()\n",
        "\n",
        "X_train = np.concatenate([X_train,batch_x[0]])\n",
        "y_train = np.concatenate([y_train,batch_x[1]])\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "ACpfNOVRJ0s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "前回のモデル作成時に行ったマスク処理を施す。"
      ],
      "metadata": {
        "id": "YX8Wb_fWMoCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_masked = X_train.copy()\n",
        "X_test_masked = X_test.copy()\n",
        "for i in range(X_train.shape[0]):\n",
        "    X_train_masked[i][outer_disk_mask] = 0\n",
        "for i in range(X_test.shape[0]):\n",
        "    X_test_masked[i][outer_disk_mask] = 0"
      ],
      "metadata": {
        "id": "Zvk9fvsH5sO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train,X_test,outer_disk_mask\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "misDuMf-8ce_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "グレースケール化を施す。"
      ],
      "metadata": {
        "id": "G5Kfw-Z1MwBC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLruPHdxdvSq"
      },
      "outputs": [],
      "source": [
        "X_train_masked_gray = np.empty((X_train_masked.shape[0],32,32))\n",
        "\n",
        "for i in range(X_train_masked.shape[0]):\n",
        "    im_gray = cv2.cvtColor(X_train_masked[i], cv2.COLOR_BGR2GRAY)\n",
        "    X_train_masked_gray[i] = im_gray\n",
        "\n",
        "print(\"X_train_masked_gray's shape is\",X_train_masked_gray.shape)\n",
        "\n",
        "\n",
        "X_test_masked_gray = np.empty((X_test_masked.shape[0],X_test_masked.shape[1],X_test_masked.shape[2]))\n",
        "\n",
        "for i in range(X_test_masked.shape[0]):\n",
        "    im_gray = cv2.cvtColor(X_test_masked[i], cv2.COLOR_BGR2GRAY)\n",
        "    X_test_masked_gray[i] = im_gray\n",
        "\n",
        "print(\"X_test_masked_gray's shape is\",X_test_masked_gray.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train_masked,X_test_masked\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "NvqsBgolSkEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wRbIBNb9Wwi"
      },
      "outputs": [],
      "source": [
        "#データをkerasが読み取れる形式に変換する(one-hotベクトル)\n",
        "y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
        "print(\"y_train's shape after change to one-hot vec is\",y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checkpointの設定\n",
        "checkpoint_path,cp_callback = create_checkpoint(\"重みを保存するファイルのパスをここに記載\")"
      ],
      "metadata": {
        "id": "qTS0zYT0_xs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e34vtMNodvSt"
      },
      "outputs": [],
      "source": [
        "#https://qiita.com/yy1003/items/c590d1a26918e4abe512やhttps://deepage.net/deep_learning/2016/10/26/batch_normalization.html参照\n",
        "\n",
        "NUMBER_OF_EPOCH = 32\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "input_size  = (img_rows,img_cols,1)\n",
        "\n",
        "model_masked_gray = create_2Dmodel(input_size)\n",
        "\n",
        "model_masked_gray.fit(X_train_masked_gray[split_index[0][0]], y_train[split_index[0][0]],\n",
        "                      batch_size=BATCH_SIZE,\n",
        "                      epochs=NUMBER_OF_EPOCH,\n",
        "                      verbose=1,\n",
        "                      validation_data=(X_train_masked_gray[split_index[0][1]], y_train[split_index[0][1]]),\n",
        "                      callbacks=[cp_callback])\n",
        "\n",
        "plot_history_of_model(model_masked_gray)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#model作成\n",
        "input_size  = (img_rows,img_cols,1)\n",
        "model_masked_gray = create_2Dmodel(input_size)\n",
        "#保存済み重みのロード\n",
        "model_masked_gray.load_weights(checkpoint_path)\n",
        "\n",
        "loss, acc = model_masked_gray.evaluate(X_test_masked_gray, y_test, verbose=2)\n",
        "print(\"Untrained model, accuracy: {:5.2f}%\".format(100 * acc))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3QnwHB3jWhcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train_masked_gray\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "EN7jJuLD8qgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3pvaT5B8QFR"
      },
      "outputs": [],
      "source": [
        "pred_masked_gray = model_masked_gray.predict(X_test_masked_gray)\n",
        "\n",
        "count = 0\n",
        "for i in range(len(X_test_masked_gray)):\n",
        "    if np.argmax(pred_masked_gray[i]) == np.argmax(y_test[i]):\n",
        "        count +=1\n",
        "\n",
        "print(\"accuracy :\",count/len(X_test_masked_gray))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del X_test_masked_gray,model_masked_gray\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "LHuTzmzr80Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 元データをグレースケール化したデータの分類モデル"
      ],
      "metadata": {
        "id": "YFGyssor-d5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train,y_train),(X_test,y_test) = load()"
      ],
      "metadata": {
        "id": "DmSkj7Tf17gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_cat = X_train[np.where(y_train == 3)[0]]\n",
        "\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "params = {\"zoom_range\" : 0.3,\n",
        "        \"horizontal_flip\" : True,\n",
        "        \"rotation_range\" : 30,\n",
        "        \"height_shift_range\": 0.1,\n",
        "        \"width_shift_range\": 0.1,\n",
        "        \"channel_shift_range\": 0.3}\n",
        "\n",
        "#データ拡張用オブジェクトの定義\n",
        "data_generator = keras.preprocessing.image.ImageDataGenerator(**params)\n",
        "\n",
        "# 同じ画像を複製する\n",
        "train_aug = X_cat.copy()\n",
        "\n",
        "y_aug = y_train[np.where(y_train == 3)[0]]\n",
        "\n",
        "#データ拡張\n",
        "generator = data_generator.flow(train_aug, y_aug, batch_size=15000)\n",
        "\n",
        "# 変換後のデータを取得\n",
        "batch_x = generator.next()\n",
        "\n",
        "X_train = np.concatenate([X_train,batch_x[0]])\n",
        "y_train = np.concatenate([y_train,batch_x[1]])\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "k61KZcBrJ20M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "グレースケール化を施す。"
      ],
      "metadata": {
        "id": "fOOtcPlKM9Dm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVoaN3u5--M_"
      },
      "outputs": [],
      "source": [
        "X_train_gray = np.empty((X_train.shape[0],32,32))\n",
        "\n",
        "for i in range(X_train.shape[0]):\n",
        "    im_gray = cv2.cvtColor(X_train[i], cv2.COLOR_BGR2GRAY)\n",
        "    X_train_gray[i] = im_gray\n",
        "\n",
        "print(\"X_train_gray's shape is\",X_train_gray.shape)\n",
        "\n",
        "\n",
        "X_test_gray = np.empty((X_test.shape[0],X_test.shape[1],X_test.shape[2]))\n",
        "\n",
        "for i in range(X_test.shape[0]):\n",
        "    im_gray = cv2.cvtColor(X_test[i], cv2.COLOR_BGR2GRAY)\n",
        "    X_test_gray[i] = im_gray\n",
        "\n",
        "print(\"X_test_gray's shape is\",X_test_gray.shape)\n",
        "\n",
        "del X_train,X_test\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy-jNmnE_TNi"
      },
      "outputs": [],
      "source": [
        "#データをkerasが読み取れる形式に変換する(one-hotベクトル)\n",
        "y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
        "print(\"y_train's shape after change to one-hot vec is\",y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path,cp_callback = create_checkpoint(\"重みを保存するファイルのパスをここに記載\")"
      ],
      "metadata": {
        "id": "KB81lrNbtrTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azuMExO07uuN"
      },
      "outputs": [],
      "source": [
        "#https://qiita.com/yy1003/items/c590d1a26918e4abe512やhttps://deepage.net/deep_learning/2016/10/26/batch_normalization.html参照\n",
        "\n",
        "NUMBER_OF_EPOCH = 32\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "input_size  = (img_rows,img_cols,1)\n",
        "\n",
        "model_gray = create_2Dmodel(input_size)\n",
        "\n",
        "model_gray.fit(X_train_gray[split_index[0][0]], y_train[split_index[0][0]],\n",
        "              batch_size=BATCH_SIZE,\n",
        "              epochs=NUMBER_OF_EPOCH,\n",
        "              verbose=1,\n",
        "              validation_data=(X_train_gray[split_index[0][1]], y_train[split_index[0][1]]),\n",
        "              callbacks=[cp_callback])\n",
        "\n",
        "plot_history_of_model(model_gray)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#保存済みの重みを読み込み\n",
        "input_size  = (img_rows,img_cols,1)\n",
        "model_gray = create_2Dmodel(input_size)\n",
        "\n",
        "model_gray.load_weights(checkpoint_path)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "f3ujXqHPt-et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train_gray,y_train\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "0K4AVfzWzC2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwLlMM3SdvSt"
      },
      "outputs": [],
      "source": [
        "pred_gray = model_gray.predict(X_test_gray)\n",
        "\n",
        "count = 0\n",
        "for i in range(len(X_test_gray)):\n",
        "    if np.argmax(pred_gray[i]) == np.argmax(y_test[i]):\n",
        "        count +=1\n",
        "\n",
        "print(\"accuracy :\",count/len(X_test_gray))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# アンサンブル学習"
      ],
      "metadata": {
        "id": "W6k6hHipwQaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "これまでの予測結果をアンサンブルすることで精度の向上を目指す。"
      ],
      "metadata": {
        "id": "ZRvDmDRpNVBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "pred_average = (pred_color + pred_color_masked + pred_gray + pred_masked_gray) / 4\n",
        "count = 0\n",
        "\n",
        "negative_list = np.zeros(10)\n",
        "pred_list = np.zeros(10)\n",
        "\n",
        "true = np.zeros(10)\n",
        "pred = np.zeros(10)\n",
        "\n",
        "my_matrix = np.zeros((10,10))\n",
        "\n",
        "for i in range(len(pred_color)):\n",
        "    if np.argmax(pred_average[i]) == np.argmax(y_test[i]):\n",
        "        count +=1\n",
        "    else:\n",
        "      negative_list[np.argmax(y_test[i])] += 1\n",
        "      pred_list[np.argmax(pred_average[i])] += 1\n",
        "\n",
        "    true[np.argmax(y_test[i])] += 1\n",
        "    pred[np.argmax(pred_average[i])] += 1\n",
        "\n",
        "    my_matrix[np.argmax(y_test[i])][np.argmax(pred_average[i])] += 1\n",
        "\n",
        "print(\"accuracy :\",count/len(pred_color))\n",
        "print(label_dict)\n",
        "print(negative_list)\n",
        "print(pred_list,\"\\n\")\n",
        "\n",
        "print(true)\n",
        "print(pred,\"\\n\")\n",
        "\n",
        "print(my_matrix)"
      ],
      "metadata": {
        "id": "QnnR93TN5RpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記のコードで出力した混同行列を視覚的に理解しやすく成形する。"
      ],
      "metadata": {
        "id": "EJTgwiWPNeJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['飛行機', '自動車', '鳥', '猫', '鹿', '犬', 'カエル', '馬', '船', 'トラック']"
      ],
      "metadata": {
        "id": "qSt0iqSCozok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data=my_matrix, index=labels, columns=labels)"
      ],
      "metadata": {
        "id": "rPZHFwKKpL41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "tzLaCe3opY0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install japanize_matplotlib"
      ],
      "metadata": {
        "id": "MXFI0BcupfFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import japanize_matplotlib"
      ],
      "metadata": {
        "id": "y9ZIMGCCp0Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (12,9))\n",
        "plt.xlabel(\"pred\")\n",
        "plt.ylabel(\"true\")\n",
        "sns.heatmap(df, annot=True, cmap=\"hot\",fmt='.0f')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_WfXZjp3lFtk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}